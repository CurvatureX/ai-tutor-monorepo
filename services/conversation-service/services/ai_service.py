import asyncio
import json
import uuid
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, AsyncGenerator
import httpx
from models.openai_models import ChatMessage, TutorSession
from config.settings import Settings


class AIService:
    """AI service for handling conversation generation and tutoring logic using local models"""

    def __init__(self, settings: Settings):
        self.settings = settings
        self.local_model = None
        self.tokenizer = None
        self.sessions_store = {}  # In production, use Redis or database
        self.conversation_history = {}  # In production, use Redis or database

    async def initialize(self):
        """Initialize AI service with local models"""
        try:
            if self.settings.use_local_model:
                await self._load_local_model()
            print("âœ… AI Service initialized successfully")
        except Exception as e:
            print(f"âŒ Failed to initialize AI Service: {e}")
            # Continue without local model for demo purposes
            print("âš ï¸  Running without local model - using simulated responses")

    async def _load_local_model(self):
        """Load local AI model (optional)"""
        try:
            # è¿™é‡Œå¯ä»¥åŠ è½½ä½ çš„æœ¬åœ°æ¨¡åž‹
            # ä¾‹å¦‚ï¼štransformers, llamacpp, æˆ–å…¶ä»–æ¨¡åž‹
            print("ðŸ”„ Loading local AI model...")

            # ç¤ºä¾‹ï¼šä½¿ç”¨ Hugging Face transformers
            # from transformers import AutoTokenizer, AutoModelForCausalLM
            # self.tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")
            # self.local_model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")

            # æš‚æ—¶è·³è¿‡çœŸå®žæ¨¡åž‹åŠ è½½ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå“åº”
            print("ðŸ“ Using simulated responses for demo")

        except Exception as e:
            print(f"âš ï¸  Could not load local model: {e}")
            print("ðŸ“ Falling back to simulated responses")

    async def health_check(self) -> bool:
        """Check if AI service is healthy"""
        return True

    async def cleanup(self):
        """Cleanup resources"""
        if self.local_model:
            del self.local_model
        if self.tokenizer:
            del self.tokenizer

    async def create_completion(
        self,
        messages: List[ChatMessage],
        model: str,
        user_id: str,
        temperature: Optional[float] = 0.7,
        max_tokens: Optional[int] = None,
        stream: Optional[bool] = False,
    ) -> Dict[str, Any]:
        """Create a chat completion using local AI models"""

        try:
            # Set default values for None parameters
            temp = temperature if temperature is not None else 0.7
            streaming = stream if stream is not None else False
            max_tok = max_tokens if max_tokens is not None else 500

            # Convert messages to local format
            conversation_text = self._format_messages_for_model(messages)

            if streaming:
                return await self._create_streaming_completion(
                    conversation_text, model, temp, max_tok, user_id
                )
            else:
                return await self._create_standard_completion(
                    conversation_text, model, temp, max_tok, user_id
                )

        except Exception as e:
            print(f"Error creating completion: {e}")
            raise e

    def _format_messages_for_model(self, messages: List[ChatMessage]) -> str:
        """Convert OpenAI message format to text for local model"""
        formatted_messages = []

        for msg in messages:
            if msg.role.value == "system":
                formatted_messages.append(f"System: {msg.content}")
            elif msg.role.value == "user":
                formatted_messages.append(f"Human: {msg.content}")
            elif msg.role.value == "assistant":
                formatted_messages.append(f"Assistant: {msg.content}")

        # Add system prompt for tutoring
        system_prompt = f"System: {self._get_tutor_system_prompt()}"
        formatted_messages.insert(0, system_prompt)

        return "\n".join(formatted_messages) + "\nAssistant:"

    async def _create_standard_completion(
        self,
        conversation_text: str,
        model: str,
        temperature: float,
        max_tokens: int,
        user_id: str,
    ) -> Dict[str, Any]:
        """Create standard (non-streaming) completion"""

        if self.local_model and self.tokenizer:
            # ä½¿ç”¨çœŸå®žçš„æœ¬åœ°æ¨¡åž‹
            response_text = await self._generate_with_local_model(
                conversation_text, temperature, max_tokens
            )
        else:
            # ä½¿ç”¨æ™ºèƒ½æ¨¡æ‹Ÿå“åº”
            response_text = await self._generate_simulated_response(
                conversation_text, temperature
            )

        # è®¡ç®— token æ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼‰
        prompt_tokens = len(conversation_text.split())
        completion_tokens = len(response_text.split())

        return {
            "content": response_text,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": prompt_tokens + completion_tokens,
        }

    async def _create_streaming_completion(
        self,
        conversation_text: str,
        model: str,
        temperature: float,
        max_tokens: int,
        user_id: str,
    ) -> Dict[str, Any]:
        """Create streaming completion"""

        if self.local_model and self.tokenizer:
            stream = self._generate_streaming_with_local_model(
                conversation_text, temperature, max_tokens
            )
        else:
            stream = self._generate_simulated_streaming_response(
                conversation_text, temperature
            )

        return {"stream": stream}

    async def _generate_with_local_model(
        self, conversation_text: str, temperature: float, max_tokens: int
    ) -> str:
        """Generate response using local AI model"""
        try:
            # è¿™é‡Œå®žçŽ°çœŸå®žçš„æœ¬åœ°æ¨¡åž‹æŽ¨ç†
            # ç¤ºä¾‹ä»£ç ï¼ˆéœ€è¦æ ¹æ®ä½ çš„æ¨¡åž‹è°ƒæ•´ï¼‰:
            """
            inputs = self.tokenizer.encode(conversation_text, return_tensors="pt")

            with torch.no_grad():
                outputs = self.local_model.generate(
                    inputs,
                    max_length=inputs.shape[1] + max_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
            return response.strip()
            """

            # æš‚æ—¶è¿”å›žæ¨¡æ‹Ÿå“åº”
            return await self._generate_simulated_response(
                conversation_text, temperature
            )

        except Exception as e:
            print(f"Error in local model generation: {e}")
            return await self._generate_simulated_response(
                conversation_text, temperature
            )

    async def _generate_simulated_response(
        self, conversation_text: str, temperature: float
    ) -> str:
        """Generate intelligent simulated response for demo purposes"""

        # åˆ†æžæœ€åŽçš„ç”¨æˆ·æ¶ˆæ¯
        lines = conversation_text.strip().split("\n")
        last_human_message = ""

        for line in reversed(lines):
            if line.startswith("Human:"):
                last_human_message = line.replace("Human:", "").strip()
                break

        # åŸºäºŽå†…å®¹ç”Ÿæˆæ™ºèƒ½å“åº”
        if not last_human_message:
            return "Hello! I'm your AI tutor. How can I help you learn today?"

        # æ·»åŠ ä¸€äº›å»¶è¿Ÿæ¨¡æ‹ŸçœŸå®žå“åº”æ—¶é—´
        await asyncio.sleep(0.5 + temperature * 0.5)

        # æ™ºèƒ½å“åº”é€»è¾‘
        response = self._generate_tutoring_response(last_human_message, temperature)

        return response

    def _generate_tutoring_response(self, user_message: str, temperature: float) -> str:
        """Generate educational response based on user message"""

        message_lower = user_message.lower()

        # æ•°å­¦ç›¸å…³
        if any(
            word in message_lower
            for word in [
                "æ•°å­¦",
                "æ–¹ç¨‹",
                "å‡½æ•°",
                "å¾®ç§¯åˆ†",
                "ä»£æ•°",
                "math",
                "equation",
                "function",
            ]
        ):
            responses = [
                "è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ¥ç†è§£è¿™ä¸ªæ•°å­¦æ¦‚å¿µã€‚é¦–å…ˆï¼Œä½ èƒ½å‘Šè¯‰æˆ‘ä½ å¯¹è¿™ä¸ªé—®é¢˜çš„åˆæ­¥æƒ³æ³•å—ï¼Ÿ",
                "æ•°å­¦æ˜¯ä¸€ä¸ªéžå¸¸æœ‰è¶£çš„é¢†åŸŸï¼ä¸ºäº†æ›´å¥½åœ°å¸®åŠ©ä½ ï¼Œä½ èƒ½å…·ä½“è¯´æ˜Žä½ åœ¨å“ªä¸ªéƒ¨åˆ†é‡åˆ°å›°éš¾äº†å—ï¼Ÿ",
                "å¾ˆå¥½çš„é—®é¢˜ï¼è®©æˆ‘ä»¬ä»ŽåŸºç¡€å¼€å§‹ã€‚ä½ è§‰å¾—è§£å†³è¿™ç±»é—®é¢˜çš„ç¬¬ä¸€æ­¥åº”è¯¥æ˜¯ä»€ä¹ˆï¼Ÿ",
            ]

        # ç§‘å­¦ç›¸å…³
        elif any(
            word in message_lower
            for word in [
                "ç‰©ç†",
                "åŒ–å­¦",
                "ç”Ÿç‰©",
                "physics",
                "chemistry",
                "biology",
                "å…‰åˆä½œç”¨",
                "photosynthesis",
            ]
        ):
            responses = [
                "ç§‘å­¦æ¦‚å¿µæœ€å¥½é€šè¿‡å®žä¾‹æ¥ç†è§£ã€‚è®©æˆ‘ç»™ä½ ä¸¾ä¸ªæ—¥å¸¸ç”Ÿæ´»ä¸­çš„ä¾‹å­æ¥è¯´æ˜Žè¿™ä¸ªæ¦‚å¿µ...",
                "è¿™æ˜¯ä¸€ä¸ªå¾ˆé‡è¦çš„ç§‘å­¦æ¦‚å¿µï¼ä½ èƒ½å…ˆæè¿°ä¸€ä¸‹ä½ å·²ç»çŸ¥é“çš„ç›¸å…³å†…å®¹å—ï¼Ÿ",
                "ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™ä¸ªçŽ°è±¡ï¼Œæˆ‘ä»¬å¯ä»¥ä»Žå®ƒçš„åŸºæœ¬åŽŸç†å¼€å§‹ã€‚ä½ è§‰å¾—ä»€ä¹ˆæ˜¯æœ€é‡è¦çš„å› ç´ ï¼Ÿ",
            ]

        # ç¼–ç¨‹ç›¸å…³
        elif any(
            word in message_lower
            for word in ["ç¼–ç¨‹", "ä»£ç ", "programming", "code", "python", "javascript"]
        ):
            responses = [
                "ç¼–ç¨‹çš„å…³é”®æ˜¯ç†è§£é€»è¾‘å’Œæ­¥éª¤ã€‚è®©æˆ‘ä»¬åˆ†è§£è¿™ä¸ªé—®é¢˜ï¼Œä¸€æ­¥ä¸€æ­¥æ¥è§£å†³ã€‚",
                "å¾ˆå¥½çš„ç¼–ç¨‹é—®é¢˜ï¼ä½ èƒ½å…ˆå‘Šè¯‰æˆ‘ä½ æƒ³è¦å®žçŽ°ä»€ä¹ˆåŠŸèƒ½å—ï¼Ÿ",
                "ç¼–ç¨‹æ—¶æœ€é‡è¦çš„æ˜¯æ€è·¯æ¸…æ™°ã€‚æˆ‘ä»¬å…ˆä»Žä¼ªä»£ç å¼€å§‹ï¼Œä½ è§‰å¾—ç¬¬ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆï¼Ÿ",
            ]

        # è¯­è¨€å­¦ä¹ 
        elif any(
            word in message_lower
            for word in ["è¯­æ³•", "å•è¯", "grammar", "vocabulary", "è‹±è¯­", "english"]
        ):
            responses = [
                "è¯­è¨€å­¦ä¹ éœ€è¦å¤šç»ƒä¹ ã€‚è®©æˆ‘ä»¬ä»Žè¿™ä¸ªå…·ä½“çš„ä¾‹å­å¼€å§‹ï¼Œä½ èƒ½è¯•ç€é€ ä¸ªå¥å­å—ï¼Ÿ",
                "ç†è§£è¯­æ³•è§„åˆ™å¾ˆé‡è¦ï¼Œä½†æ›´é‡è¦çš„æ˜¯åœ¨å®žé™…ä¸­åº”ç”¨ã€‚ä½ æƒ³å…ˆä»Žå“ªä¸ªæ–¹é¢å¼€å§‹ï¼Ÿ",
                "è¯­è¨€å­¦ä¹ çš„ç§˜è¯€æ˜¯å¤šä½¿ç”¨ã€‚ä½ èƒ½ç”¨åˆšå­¦çš„è¿™ä¸ªçŸ¥è¯†ç‚¹æ¥å›žç­”æˆ‘ä¸€ä¸ªé—®é¢˜å—ï¼Ÿ",
            ]

        # é€šç”¨é—®å€™
        elif any(
            word in message_lower for word in ["ä½ å¥½", "hello", "hi", "å¸®åŠ©", "help"]
        ):
            responses = [
                "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„AIå­¦ä¹ åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å¸®ä½ å­¦ä¹ å„ç§å­¦ç§‘ï¼ŒåŒ…æ‹¬æ•°å­¦ã€ç§‘å­¦ã€ç¼–ç¨‹ç­‰ã€‚ä½ ä»Šå¤©æƒ³å­¦ä»€ä¹ˆï¼Ÿ",
                "å¾ˆé«˜å…´è§åˆ°ä½ ï¼æˆ‘ä¸“é—¨å¸®åŠ©å­¦ç”Ÿå­¦ä¹ å’Œç†è§£å„ç§æ¦‚å¿µã€‚ä½ æœ‰ä»€ä¹ˆå­¦ä¹ ç›®æ ‡å—ï¼Ÿ",
                "æ¬¢è¿Žï¼æˆ‘æ˜¯ä¸€ä½AIæ•™å¸ˆï¼Œæ“…é•¿ç”¨è‹æ ¼æ‹‰åº•å¼æ•™å­¦æ³•å¸®åŠ©å­¦ç”Ÿæ€è€ƒã€‚ä½ é‡åˆ°ä»€ä¹ˆå­¦ä¹ æŒ‘æˆ˜äº†å—ï¼Ÿ",
            ]

        # é»˜è®¤å“åº”
        else:
            responses = [
                "è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰è¶£çš„é—®é¢˜ï¼è®©æˆ‘ä»¬ä¸€èµ·æŽ¢ç´¢ä¸€ä¸‹ã€‚ä½ èƒ½åˆ†äº«æ›´å¤šçš„èƒŒæ™¯ä¿¡æ¯å—ï¼Ÿ",
                "æˆ‘ç†è§£ä½ çš„ç–‘é—®ã€‚ä¸ºäº†ç»™ä½ æœ€å¥½çš„å¸®åŠ©ï¼Œä½ èƒ½å‘Šè¯‰æˆ‘ä½ çš„å­¦ä¹ ç›®æ ‡æ˜¯ä»€ä¹ˆå—ï¼Ÿ",
                "è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ¥åˆ†æžè¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œä½ è§‰å¾—å…³é”®ç‚¹åœ¨å“ªé‡Œï¼Ÿ",
                "å¾ˆå¥½çš„é—®é¢˜ï¼æˆ‘å»ºè®®æˆ‘ä»¬ä»ŽåŸºç¡€æ¦‚å¿µå¼€å§‹ã€‚ä½ å¯¹ç›¸å…³çš„åŸºæœ¬çŸ¥è¯†äº†è§£å¤šå°‘ï¼Ÿ",
            ]

        # æ ¹æ® temperature é€‰æ‹©å“åº”çš„éšæœºæ€§
        import random

        if temperature > 0.7:
            # é«˜ temperatureï¼Œæ›´æœ‰åˆ›æ„çš„å“åº”
            selected_response = random.choice(responses)
            # æ·»åŠ ä¸€äº›å˜åŒ–
            endings = [
                " æˆ‘å¾ˆæœŸå¾…å¬åˆ°ä½ çš„æƒ³æ³•ï¼",
                " è®°ä½ï¼Œå­¦ä¹ æ˜¯ä¸€ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬æ…¢æ…¢æ¥ã€‚",
                " ä½ çš„å¥½å¥‡å¿ƒå¾ˆæ£’ï¼Œè¿™æ˜¯å­¦ä¹ çš„å…³é”®ï¼",
                " è®©æˆ‘ä»¬ä¸€èµ·æŽ¢ç´¢è¿™ä¸ªæœ‰è¶£çš„é¢†åŸŸã€‚",
            ]
            selected_response += random.choice(endings)
        else:
            # ä½Ž temperatureï¼Œæ›´ä¸€è‡´çš„å“åº”
            selected_response = responses[0]

        return selected_response

    async def _generate_simulated_streaming_response(
        self, conversation_text: str, temperature: float
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Generate simulated streaming response"""

        response_text = await self._generate_simulated_response(
            conversation_text, temperature
        )
        words = response_text.split()

        chunk_id = f"chatcmpl-{str(uuid.uuid4())}"
        created = int(time.time())

        # é€è¯å‘é€
        for i, word in enumerate(words):
            chunk_data = {
                "id": chunk_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": "local-tutor",
                "choices": [
                    {
                        "index": 0,
                        "delta": {"content": word + " "},
                        "finish_reason": None,
                    }
                ],
            }
            yield chunk_data
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿæµå¼å“åº”å»¶è¿Ÿ

        # å‘é€ç»“æŸæ ‡è®°
        final_chunk = {
            "id": chunk_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": "local-tutor",
            "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
        }
        yield final_chunk

    async def _generate_streaming_with_local_model(
        self, conversation_text: str, temperature: float, max_tokens: int
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Generate streaming response using local model"""
        # è¿™é‡Œå®žçŽ°çœŸå®žçš„æµå¼æœ¬åœ°æ¨¡åž‹æŽ¨ç†
        # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿå“åº”
        async for chunk in self._generate_simulated_streaming_response(
            conversation_text, temperature
        ):
            yield chunk

    def _get_tutor_system_prompt(self) -> str:
        """Get the system prompt for AI tutoring"""
        return """You are an expert AI tutor designed to help students learn effectively. Your role is to:

1. **Understand the student's level**: Adapt your explanations to their knowledge level
2. **Ask guiding questions**: Help students discover answers rather than giving them directly
3. **Provide clear explanations**: Use simple language and examples when needed
4. **Encourage critical thinking**: Challenge students to think deeper about concepts
5. **Be patient and supportive**: Create a positive learning environment
6. **Check understanding**: Regularly verify that students understand before moving forward
7. **Provide practical examples**: Use real-world applications to illustrate concepts

Guidelines:
- Always maintain a friendly, encouraging tone
- Break down complex topics into manageable parts
- Use the Socratic method when appropriate
- Celebrate student progress and breakthroughs
- Adapt to different learning styles
- If a student seems frustrated, provide more guidance and support
- End responses with a question or suggestion for the next step when appropriate

Remember: Your goal is to facilitate learning, not just provide answers."""

    async def create_tutor_session(
        self, user_id: str, subject: str, level: str = "intermediate"
    ) -> TutorSession:
        """Create a new tutoring session"""

        session_id = str(uuid.uuid4())
        timestamp = datetime.utcnow().isoformat()

        session = TutorSession(
            id=session_id,
            user_id=user_id,
            subject=subject,
            level=level,
            created_at=timestamp,
            last_activity=timestamp,
        )

        # Store session (in production, use proper database)
        self.sessions_store[session_id] = session.dict()

        # Initialize conversation history
        self.conversation_history[session_id] = []

        return session

    async def get_user_sessions(self, user_id: str) -> List[TutorSession]:
        """Get all sessions for a user"""

        user_sessions = []
        for session_data in self.sessions_store.values():
            if session_data["user_id"] == user_id:
                user_sessions.append(TutorSession(**session_data))

        return user_sessions

    async def get_session_history(self, session_id: str, user_id: str) -> List[Dict]:
        """Get conversation history for a session"""

        # Verify session belongs to user
        session = self.sessions_store.get(session_id)
        if not session or session["user_id"] != user_id:
            raise ValueError("Session not found or access denied")

        return self.conversation_history.get(session_id, [])

    async def add_message_to_session(self, session_id: str, message: ChatMessage):
        """Add a message to session history"""

        if session_id not in self.conversation_history:
            self.conversation_history[session_id] = []

        self.conversation_history[session_id].append(
            {
                "role": message.role.value,
                "content": message.content,
                "timestamp": datetime.utcnow().isoformat(),
            }
        )

        # Update session last activity
        if session_id in self.sessions_store:
            self.sessions_store[session_id][
                "last_activity"
            ] = datetime.utcnow().isoformat()
            self.sessions_store[session_id]["message_count"] += 1
