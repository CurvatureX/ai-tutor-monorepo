# AI Tutor Conversation Service

ğŸ¤– **OpenAI-compatible API service** that provides AI tutoring conversations using **local models** instead of calling OpenAI API. Built with FastAPI and managed by uv.

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- **ğŸ”„ OpenAI API å®Œå…¨å…¼å®¹**: ç°æœ‰ä½¿ç”¨ OpenAI SDK çš„ä»£ç å¯ä»¥ç›´æ¥åˆ‡æ¢åˆ°æˆ‘ä»¬çš„æœåŠ¡
- **ğŸ  æœ¬åœ°æ¨¡å‹æ”¯æŒ**: ä¸ä¾èµ–å¤–éƒ¨ APIï¼Œå¯ä»¥ä½¿ç”¨è‡ªå·±çš„ AI æ¨¡å‹
- **ğŸ“ æ™ºèƒ½æ•™å­¦å“åº”**: ä¸“é—¨ä¸ºæ•™è‚²åœºæ™¯ä¼˜åŒ–çš„ AI å“åº”
- **âš¡ uv ç®¡ç†**: ä½¿ç”¨æœ€ç°ä»£çš„ Python åŒ…ç®¡ç†å·¥å…·

## âš¡ Quick Start with uv

### 1. Install uv (if not already installed)

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. Setup and Run

```bash
# Clone and navigate to the service directory
cd services/conversation-service

# Install all dependencies (including dev dependencies)
uv sync

# Run the service
uv run python main.py

# Or use the development script
uv run python scripts/dev.py run
```

### 3. Test the Service

```bash
# Visit API documentation
open http://localhost:8000/docs

# Test health endpoint
curl http://localhost:8000/health

# Or run the comprehensive test
uv run python test_openai_client.py
```

## ğŸ”Œ OpenAI SDK å…¼å®¹æ€§

ç°æœ‰çš„ OpenAI ä»£ç å¯ä»¥ç›´æ¥åˆ‡æ¢åˆ°æˆ‘ä»¬çš„æœåŠ¡ï¼š

```python
import openai

# åªéœ€è¦æ”¹å˜è¿™ä¸¤è¡Œé…ç½®
openai.api_base = "http://localhost:8000/v1"  # æŒ‡å‘æˆ‘ä»¬çš„æœåŠ¡
openai.api_key = "dev-token"                  # ä½¿ç”¨æˆ‘ä»¬çš„è®¤è¯

# å…¶ä½™ä»£ç å®Œå…¨ä¸å˜ï¼
response = openai.ChatCompletion.create(
    model="gpt-4-tutor",
    messages=[
        {"role": "user", "content": "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"}
    ]
)

print(response.choices[0].message.content)
```

## ğŸ› ï¸ Development Commands

```bash
# Install dependencies
uv sync                              # Production only
uv run python scripts/dev.py dev    # All dependencies

# Run service
uv run python scripts/dev.py run

# Code quality
uv run python scripts/dev.py lint   # Check code
uv run python scripts/dev.py format # Format code

# Testing
uv run python scripts/dev.py test   # Run tests
uv run python test_openai_client.py # Test OpenAI compatibility

# Utilities
uv run python scripts/dev.py clean  # Clean cache
```

## ğŸ“Š API å…¼å®¹æ€§

### âœ… æ”¯æŒçš„ OpenAI API ç«¯ç‚¹

| ç«¯ç‚¹                        | çŠ¶æ€        | è¯´æ˜                 |
| --------------------------- | ----------- | -------------------- |
| `GET /v1/models`            | âœ… å®Œå…¨å…¼å®¹ | åˆ—å‡ºå¯ç”¨æ¨¡å‹         |
| `POST /v1/chat/completions` | âœ… å®Œå…¨å…¼å®¹ | èŠå¤©å®Œæˆï¼ˆæ”¯æŒæµå¼ï¼‰ |

### ğŸ“ æ•™å­¦ä¸“ç”¨ç«¯ç‚¹

| ç«¯ç‚¹                          | åŠŸèƒ½             |
| ----------------------------- | ---------------- |
| `POST /v1/tutor/session`      | åˆ›å»ºæ•™å­¦ä¼šè¯     |
| `GET /v1/tutor/sessions`      | è·å–ç”¨æˆ·ä¼šè¯åˆ—è¡¨ |
| `GET /v1/tutor/sessions/{id}` | è·å–ä¼šè¯å†å²     |

## ğŸ¤– æ™ºèƒ½æ•™å­¦åŠŸèƒ½

æˆ‘ä»¬çš„ AI å…·æœ‰ä¸“é—¨çš„æ•™å­¦èƒ½åŠ›ï¼š

- **ğŸ§  è‹æ ¼æ‹‰åº•å¼æ•™å­¦**: é€šè¿‡é—®é¢˜å¼•å¯¼å­¦ç”Ÿæ€è€ƒ
- **ğŸ“š å¤šå­¦ç§‘æ”¯æŒ**: æ•°å­¦ã€ç§‘å­¦ã€ç¼–ç¨‹ã€è¯­è¨€ç­‰
- **ğŸ¯ è‡ªé€‚åº”éš¾åº¦**: æ ¹æ®å­¦ç”Ÿæ°´å¹³è°ƒæ•´è§£é‡Šæ·±åº¦
- **ğŸ’¡ ä¸¾ä¾‹è¯´æ˜**: ä½¿ç”¨ç”Ÿæ´»å®ä¾‹å¸®åŠ©ç†è§£
- **ğŸ”„ äº’åŠ¨å¼å­¦ä¹ **: é¼“åŠ±å­¦ç”Ÿå‚ä¸å’Œæé—®

## ğŸ”§ é…ç½®é€‰é¡¹

### Environment Variables

| Variable           | Description           | Default                    |
| ------------------ | --------------------- | -------------------------- |
| `PORT`             | Server port           | `8000`                     |
| `DEBUG`            | Debug mode            | `false`                    |
| `USE_LOCAL_MODEL`  | Enable local AI model | `false`                    |
| `MODEL_PATH`       | Path to local model   | `None`                     |
| `MODEL_NAME`       | Model identifier      | `local-tutor`              |
| `JWT_SECRET`       | JWT signing secret    | `your-secret-key`          |
| `USER_SERVICE_URL` | User service URL      | `http://user-service:8001` |
| `REDIS_URL`        | Redis connection URL  | `redis://localhost:6379`   |

### æœ¬åœ°æ¨¡å‹é›†æˆ

```python
# åœ¨ ai_service.py ä¸­å¯ç”¨çœŸå®æ¨¡å‹
async def _load_local_model(self):
    from transformers import AutoTokenizer, AutoModelForCausalLM

    self.tokenizer = AutoTokenizer.from_pretrained("your-model-name")
    self.local_model = AutoModelForCausalLM.from_pretrained("your-model-name")
```

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€èŠå¤©

```bash
curl -X POST "http://localhost:8000/v1/chat/completions" \
  -H "Authorization: Bearer dev-token" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4-tutor",
    "messages": [{"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯é€’å½’"}],
    "temperature": 0.7
  }'
```

### 2. æµå¼å“åº”

```python
import httpx

async def stream_chat():
    async with httpx.AsyncClient() as client:
        async with client.stream(
            "POST", "http://localhost:8000/v1/chat/completions",
            headers={"Authorization": "Bearer dev-token"},
            json={
                "model": "local-tutor",
                "messages": [{"role": "user", "content": "è§£é‡Šæœºå™¨å­¦ä¹ "}],
                "stream": True
            }
        ) as response:
            async for chunk in response.aiter_text():
                print(chunk, end="")
```

### 3. Python å®¢æˆ·ç«¯

```python
import httpx
import asyncio

class AITutorClient:
    def __init__(self, base_url="http://localhost:8000", api_key="dev-token"):
        self.base_url = base_url
        self.headers = {"Authorization": f"Bearer {api_key}"}

    async def chat(self, message: str, model="gpt-4-tutor"):
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.base_url}/v1/chat/completions",
                headers=self.headers,
                json={
                    "model": model,
                    "messages": [{"role": "user", "content": message}]
                }
            )
            return response.json()["choices"][0]["message"]["content"]

# ä½¿ç”¨ç¤ºä¾‹
client = AITutorClient()
answer = await client.chat("ä»€ä¹ˆæ˜¯ç®—æ³•å¤æ‚åº¦ï¼Ÿ")
print(answer)
```

## ğŸ³ Docker éƒ¨ç½²

```bash
# æ„å»ºé•œåƒ
docker build -t ai-tutor-service .

# è¿è¡Œå®¹å™¨
docker run -p 8000:8000 \
  -e USE_LOCAL_MODEL=false \
  -e DEBUG=false \
  ai-tutor-service
```

## ğŸ“ˆ æ€§èƒ½å’Œæ‰©å±•

- **âš¡ é«˜æ€§èƒ½**: FastAPI + async/await
- **ğŸ”„ å¯æ‰©å±•**: æ”¯æŒè´Ÿè½½å‡è¡¡å’Œæ°´å¹³æ‰©å±•
- **ğŸ“Š ç›‘æ§**: å†…ç½®å¥åº·æ£€æŸ¥å’ŒæŒ‡æ ‡
- **ğŸ”’ å®‰å…¨**: JWT è®¤è¯å’Œ CORS é…ç½®

## ğŸ†š ä¸ OpenAI API çš„å¯¹æ¯”

| ç‰¹æ€§       | OpenAI API    | æˆ‘ä»¬çš„æœåŠ¡  |
| ---------- | ------------- | ----------- |
| API æ ¼å¼   | âœ…            | âœ… å®Œå…¨å…¼å®¹ |
| è´¹ç”¨       | ğŸ’° æŒ‰ä½¿ç”¨ä»˜è´¹ | ğŸ†“ å®Œå…¨å…è´¹ |
| æ•°æ®éšç§   | âš ï¸ å‘é€åˆ°å¤–éƒ¨ | ğŸ”’ å®Œå…¨æœ¬åœ° |
| è‡ªå®šä¹‰æ¨¡å‹ | âŒ ä¸æ”¯æŒ     | âœ… å®Œå…¨æ”¯æŒ |
| æ•™å­¦ä¼˜åŒ–   | âŒ é€šç”¨æ¨¡å‹   | ğŸ“ ä¸“é—¨ä¼˜åŒ– |
| ç¦»çº¿ä½¿ç”¨   | âŒ éœ€è¦ç½‘ç»œ   | âœ… æ”¯æŒç¦»çº¿ |

## ğŸ”„ è¿ç§»æŒ‡å—

### ä» OpenAI API è¿ç§»

1. **æ— éœ€ä¿®æ”¹ä»£ç **: åªéœ€æ›´æ”¹ `api_base` å’Œ `api_key`
2. **æ¨¡å‹æ˜ å°„**: `gpt-4` â†’ `gpt-4-tutor`, `gpt-3.5-turbo` â†’ `gpt-3.5-turbo-tutor`
3. **é¢å¤–åŠŸèƒ½**: å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„æ•™å­¦ä¸“ç”¨ç«¯ç‚¹

### ç¤ºä¾‹è¿ç§»

```python
# åŸæ¥çš„ä»£ç 
import openai
openai.api_key = "sk-..."  # OpenAI å¯†é’¥

# è¿ç§»åçš„ä»£ç 
import openai
openai.api_base = "http://your-service:8000/v1"  # ä½ çš„æœåŠ¡åœ°å€
openai.api_key = "your-jwt-token"               # ä½ çš„è®¤è¯ä»¤ç‰Œ

# å…¶ä½™ä»£ç å®Œå…¨ä¸å˜ï¼
```

## ğŸ¤ è´¡çŒ®

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯: `git checkout -b feature/amazing-feature`
3. æäº¤æ›´æ”¹: `git commit -m 'Add amazing feature'`
4. æ¨é€åˆ†æ”¯: `git push origin feature/amazing-feature`
5. åˆ›å»º Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- FastAPI å›¢é˜Ÿæä¾›ä¼˜ç§€çš„ Web æ¡†æ¶
- Hugging Face æä¾› transformers åº“
- OpenAI æä¾› API æ ‡å‡†å‚è€ƒ
